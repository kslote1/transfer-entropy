{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e8ba1539-d28d-483e-885f-54ff6253d922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09885128772440951, -0.014221854912478515]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma, atleast_2d, pi, sqrt, sum, transpose\n",
    "\n",
    "\n",
    "def get_entropy(df, bins=None):\n",
    "    pdf = pdf_histogram(df, bins)\n",
    "    ## log base 2 returns H(X) in bits\n",
    "    return -np.sum( pdf * ma.log2(pdf).filled(0)) \n",
    "\n",
    "\n",
    "def pdf_histogram(df, bins):\n",
    "\n",
    "    ordered_bins = [sorted(bins[key]) for key in sorted(bins.keys())]\n",
    "    hist, dedges = np.histogram(df.dropna().values, bins=2)#ordered_bins[0])\n",
    "    return hist/hist.sum()\n",
    "\n",
    "\n",
    "def sigma_bins(df, lags, max_bins=2):\n",
    "    \"\"\" \n",
    "    Returns bins for N-dimensional data, using standard deviation binning: each \n",
    "    bin is one S.D in width, with bins centered on the mean. Where outliers exist \n",
    "    beyond the maximum number of SDs dictated by the max_bins parameter, the\n",
    "    bins are extended to minimum/maximum values to ensure all data points are\n",
    "    captured. This may mean larger bins in the tails, and up to two bins \n",
    "    greater than the max_bins parameter suggests in total (in the unlikely case of huge\n",
    "    outliers on both sides). \n",
    "    Args:\n",
    "        max_bins        -   (int)       The maximum allowed bins in each dimension\n",
    "    Returns:\n",
    "        bins            -   (dict)      The optimal bin-edges for pdf estimation\n",
    "                                        using the histogram method, keyed by df column names\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    bins = {k:[np.mean(v)-int(max_bins/2)*np.std(v) + i * np.std(v) for i in range(max_bins)] for (k,v) in df.iteritems()}\n",
    "\n",
    "    # Since some outliers can be missed, extend bins if any points are not yet captured\n",
    "    [bins[k].append(df[k].min()) for k in df.keys() if df[k].min() < min(bins[k])]\n",
    "    [bins[k].append(df[k].max()) for k in df.keys() if df[k].max() > max(bins[k])]\n",
    "\n",
    "    bins.update({fieldname + '_lag' + str(t): edges          \n",
    "                        for (fieldname, edges) in bins.items() for t in range(lags)})\n",
    "    return bins\n",
    "\n",
    "\n",
    "def non_linear_transfer_entropy(df: pd.DataFrame, endog: str, exog: str, lags=1):\n",
    "    df = df[[exog, endog]]\n",
    "    bins = sigma_bins(df, lag)\n",
    "\n",
    "    for col_name in list(df.columns):\n",
    "        for t in range(1, lag + 1):\n",
    "            df[col_name + '_lag' + str(t)] = df[col_name].shift(t)\n",
    "\n",
    "    ## Initialise list to return TEs\n",
    "    transfer_entropies = [0, 0]\n",
    "\n",
    "    ## Require us to compare information transfer bidirectionally\n",
    "    for i, (X, Y) in enumerate({exog:endog, endog:exog}.items()):\n",
    "\n",
    "        ### Entropy calculated using Probability Density Estimation:\n",
    "            # Following: https://stat.ethz.ch/education/semesters/SS_2006/CompStat/sk-ch2.pdf\n",
    "            # Also: https://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs/lec5.pdf\n",
    "\n",
    "        ## Note Lagged Terms\n",
    "        X_lagged = X + '_lag' + str(lag - 1)\n",
    "        Y_lagged = Y + '_lag' + str(lag - 1)\n",
    "\n",
    "        ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "\n",
    "        ## 1. H(Y,Y-t,X-t)  \n",
    "        H1 = get_entropy(df = df[[Y, Y_lagged, X_lagged]], \n",
    "                        bins = {k:v for (k, v) in bins.items() if k in[Y, Y_lagged, X_lagged]})\n",
    "        ## 2. H(Y-t,X-t)\n",
    "        H2 = get_entropy(df = df[[X_lagged,Y_lagged]],\n",
    "                        bins = {k:v for (k, v) in bins.items() if k in [X_lagged, Y_lagged]}) \n",
    "        ## 3. H(Y,Y-t)  \n",
    "        H3 = get_entropy(df = df[[Y,Y_lagged]],\n",
    "                        bins =  {k: v for (k, v) in bins.items() if k in [Y, Y_lagged]})\n",
    "        ## 4. H(Y-t)  \n",
    "        H4 = get_entropy(df = df[[Y_lagged]],\n",
    "                        bins =  {k: v for (k, v) in bins.items() if k in [Y_lagged]})                \n",
    "\n",
    "\n",
    "        ### Calculate Conditonal Entropy using: H(Y|X-t,Y-t) = H(Y,X-t,Y-t) - H(X-t,Y-t)\n",
    "        conditional_entropy_joint =  H1 - H2\n",
    "\n",
    "        ### And Conditional Entropy independent of X(t) H(Y|Y-t) = H(Y,Y-t) - H(Y-t)            \n",
    "        conditional_entropy_independent = H3 - H4\n",
    "\n",
    "        ### Directional Transfer Entropy is the difference between the conditional entropies\n",
    "        transfer_entropies[i] =  conditional_entropy_independent - conditional_entropy_joint\n",
    "        \n",
    "    return transfer_entropies\n",
    "\n",
    "\n",
    "def non_linear_transfer_entropy(df: pd.DataFrame, endog: str, exog: str, lags=1):\n",
    "    df = df[[exog, endog]]\n",
    "    bins = sigma_bins(df, lag)\n",
    "\n",
    "    for col_name in list(df.columns):\n",
    "        for t in range(1, lag + 1):\n",
    "            df[col_name + '_lag' + str(t)] = df[col_name].shift(t)\n",
    "\n",
    "    ## Initialise list to return TEs\n",
    "    transfer_entropies = [0, 0]\n",
    "\n",
    "    ## Require us to compare information transfer bidirectionally\n",
    "    for i, (X, Y) in enumerate({exog:endog, endog:exog}.items()):\n",
    "\n",
    "        ### Entropy calculated using Probability Density Estimation:\n",
    "            # Following: https://stat.ethz.ch/education/semesters/SS_2006/CompStat/sk-ch2.pdf\n",
    "            # Also: https://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs/lec5.pdf\n",
    "\n",
    "        ## Note Lagged Terms\n",
    "        X_lagged = X + '_lag' + str(lag - 1)\n",
    "        Y_lagged = Y + '_lag' + str(lag - 1)\n",
    "\n",
    "        ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "\n",
    "        ## 1. H(Y,Y-t,X-t)  \n",
    "        H1 = get_entropy(df = df[[Y, Y_lagged, X_lagged]], \n",
    "                        bins = {k:v for (k, v) in bins.items() if k in[Y, Y_lagged, X_lagged]})\n",
    "        \n",
    "        ## 2. H(Y-t,X-t)\n",
    "        lagged_bins = bins = {k:v for (k, v) in bins.items() if k in [X_lagged, Y_lagged]}\n",
    "        H2 = get_entropy(df = df[[X_lagged,Y_lagged]], bins = lagged_bins)\n",
    "\n",
    "        ## 3. H(Y,Y-t)\n",
    "        lagged_bins = {k: v for (k, v) in bins.items() if k in [Y, Y_lagged]}\n",
    "        H3 = get_entropy(df = df[[Y,Y_lagged]], bins = lagged_bins)\n",
    "        ## 4. H(Y-t)  \n",
    "        lagged_bins =  {k: v for (k, v) in bins.items() if k in [Y_lagged]}\n",
    "        H4 = get_entropy(df = df[[Y_lagged]], bins = lagged_bins)                \n",
    "\n",
    "\n",
    "        ### Calculate Conditonal Entropy using: H(Y|X-t,Y-t) = H(Y,X-t,Y-t) - H(X-t,Y-t)\n",
    "        conditional_entropy_joint =  H1 - H2\n",
    "\n",
    "        ### And Conditional Entropy independent of X(t) H(Y|Y-t) = H(Y,Y-t) - H(Y-t)            \n",
    "        conditional_entropy_independent = H3 - H4\n",
    "\n",
    "        ### Directional Transfer Entropy is the difference between the conditional entropies\n",
    "        transfer_entropies[i] =  conditional_entropy_independent - conditional_entropy_joint\n",
    "    return transfer_entropies\n",
    "\n",
    "df = pd.read_csv(\"data/results_day_binned_with_states.csv\")\n",
    "endog = 'Anti-Regulation Fear-of-Regulation'\n",
    "exog = 'Daily Background Checks'\n",
    "df = df[[exog, endog]]\n",
    "\n",
    "print(non_linear_transfer_entropy(df, endog, exog, lags = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f19bed78-b605-486b-ad23-b03d245a0a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-184-d288a73b6c49>:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col_name + '_lag' + str(t)] = df[col_name].shift(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[-0.00657082565865745, -0.032248324162314296]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.01102327025004965, -0.01102327025004965]\n",
      "[-0.02091194434190466, -0.02091194434190466]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[-0.032248324162314296, -0.00657082565865745]\n",
      "[0.09885128772440951, -0.014221854912478515]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n",
      "[-0.014221854912478515, 0.09885128772440951]\n"
     ]
    }
   ],
   "source": [
    "variables = [\n",
    "    'Pro-Regulation',\n",
    "    'Anti-Regulation',\n",
    "    'Self-Defense',\n",
    "    'Fear-of-Regulation',\n",
    "    'Pro-Regulation Self-Defense',\n",
    "    'Anti-Regulation Self-defense',\n",
    "    'Pro-Regulation Fear-of-Regulation',\n",
    "    'Anti-Regulation Fear-of-Regulation',\n",
    "    'Daily Background Checks'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"data/results_day_binned_with_states.csv\")\n",
    "\n",
    "for endog in variables:\n",
    "    for exog in variables:\n",
    "        if endog == exog:\n",
    "            continue\n",
    "        te = non_linear_transfer_entropy(df, endog, exog)\n",
    "        print(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "cb9d529a-44dd-4b41-af70-34c4b4119e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Pro-Regulation', 'Anti-Regulation', 'Self-Defense', 'Fear-of-Regulation', 'Pro-Regulation Self-Defense', 'Anti-Regulation Self-defense', 'Pro-Regulation Fear-of-Regulation', 'Anti-Regulation Fear-of-Regulation', 'Daily Background Checks'])\n",
      "[[-408.29275405722103, 153.3295454545455, 4119], [-233.65133299132862, 90.94318181818181, 2408], [-111.2910499512158, 39.89772727272728, 1078], [29745, 57396.51577124973, 74427.71590909091, 116161], [-274.74096280376943, 104.34090909090907, 2824], [-226.61685153916738, 87.26136363636363, 2372], [-43.484059734240205, 16.909090909090907, 441], [-134.18886826562644, 51.25, 1391], [-242.96727587845243, 90.30681818181819, 2455]]\n",
      "[0.91035354 0.08964646] [721  71]\n"
     ]
    }
   ],
   "source": [
    "variables = [\n",
    "    'Pro-Regulation',\n",
    "    'Anti-Regulation',\n",
    "    'Self-Defense',\n",
    "    'Fear-of-Regulation',\n",
    "    'Pro-Regulation Self-Defense',\n",
    "    'Anti-Regulation Self-defense',\n",
    "    'Pro-Regulation Fear-of-Regulation',\n",
    "    'Anti-Regulation Fear-of-Regulation',\n",
    "    'Daily Background Checks'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"data/results_day_binned_with_states.csv\")\n",
    "#endog = 'Anti-Regulation Fear-of-Regulation'\n",
    "#exog = 'Daily Background Checks'\n",
    "#df = df[[exog, endog]]\n",
    "df = df[variables]\n",
    "\n",
    "lags = 1\n",
    "max_bins = 2\n",
    "bins = {k:[np.mean(v)-int(max_bins/2)*np.std(v) + i * np.std(v) for i in range(max_bins)] for (k,v) in df.iteritems()}   # Note: same as:  self.df.to_dict('list').items()}\n",
    "\n",
    "# Since some outliers can be missed, extend bins if any points are not yet captured\n",
    "[bins[k].append(df[k].min()) for k in df.keys() if df[k].min() < min(bins[k])]\n",
    "[bins[k].append(df[k].max()) for k in df.keys() if df[k].max() > max(bins[k])]\n",
    "\n",
    "# bins.update({fieldname + '_lag' + str(t): edges          \n",
    "#                     for (fieldname, edges) in bins.items() for t in range(lags)})\n",
    "ordered_bins = [sorted(bins[key]) for key in sorted(bins.keys())]\n",
    "print(bins.keys())\n",
    "print(ordered_bins)\n",
    "hist, dedges = np.histogram(df.values, 2)\n",
    "print(hist/hist.sum(), hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6142fcf6-f5c4-40dd-9ddc-15f6d4db292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_series(DF, only=None):\n",
    "    \"\"\"\n",
    "    Function to return time series shuffled rowwise along each desired column. \n",
    "    Each column is shuffled independently, removing the temporal relationship.\n",
    "    This is to calculate Z-score and Z*-score. See P. Boba et al (2015)\n",
    "    Calculated using:       df.apply(np.random.permutation)\n",
    "    Arguments:\n",
    "        df              -   (DataFrame) Time series data \n",
    "        only            -   (list)      Fieldnames to shuffle. If none, all columns shuffled \n",
    "    Returns:\n",
    "        df_shuffled     -   (DataFrame) Time series shuffled along desired columns    \n",
    "    \"\"\"\n",
    "    if not only == None:\n",
    "        shuffled_DF = DF.copy()\n",
    "        for col in only:\n",
    "            series = DF.loc[:, col].to_frame()\n",
    "            shuffled_DF[col] = series.apply(np.random.permutation)\n",
    "    else:\n",
    "        shuffled_DF = DF.apply(np.random.permutation)\n",
    "    \n",
    "    return shuffled_DF\n",
    "\n",
    "\n",
    "def significance(df, TE, endog, exog, lag=5, n_shuffles=100):\n",
    "        \"\"\"\n",
    "        Perform significance analysis on the hypothesis test of statistical causality, for both X(t)->Y(t)\n",
    "        and Y(t)->X(t) directions\n",
    "   \n",
    "        Calculated using:  Assuming stationarity, we shuffle the time series to provide the null hypothesis. \n",
    "                           The proportion of tests where TE > TE_shuffled gives the p-value significance level.\n",
    "                           The amount by which the calculated TE is greater than the average shuffled TE, divided\n",
    "                           by the standard deviation of the results, is the z-score significance level.\n",
    "        Arguments:\n",
    "            TE              -      (list)    Contains the transfer entropy in each direction, i.e. [TE_XY, TE_YX]\n",
    "            endog           -      (string)  The endogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            exog            -      (string)  The exogenous variable in the TE analysis being significance tested (i.e. X or Y) variables (giving z*-score)  \n",
    "        Returns:\n",
    "            p_value         -      Probablity of observing the result given the null hypothesis\n",
    "            z_score         -      Number of Standard Deviations result is from mean (normalised)\n",
    "        \"\"\" \n",
    "\n",
    "        ## Prepare array for Transfer Entropy of each Shuffle\n",
    "        shuffled_TEs = np.zeros(shape = (2, n_shuffles))\n",
    "\n",
    "        for i in range(n_shuffles):\n",
    "                ## Perform Shuffle\n",
    "                df = shuffle_series(df)\n",
    "                \n",
    "                ## Calculate New TE\n",
    "                \n",
    "                transfer_enthropy_shuffled = non_linear_transfer_entropy(df, endog, exog)\n",
    "                shuffled_TEs[:,i] = transfer_enthropy_shuffled\n",
    "\n",
    "        \n",
    "        ## Calculate p-values for each direction\n",
    "        p_values = (np.count_nonzero(TE[0] < shuffled_TEs[0,:]) /n_shuffles , \\\n",
    "                    np.count_nonzero(TE[1] < shuffled_TEs[1,:]) /n_shuffles)\n",
    "\n",
    "        ## Calculate z-scores for each direction\n",
    "        z_scores = ( ( TE[0] - np.mean(shuffled_TEs[0,:]) ) / np.std(shuffled_TEs[0,:]) , \\\n",
    "                     ( TE[1] - np.mean(shuffled_TEs[1,:]) ) / np.std(shuffled_TEs[1,:])  )\n",
    "        \n",
    "        TE_mean = ( np.mean(shuffled_TEs[0,:]), \\\n",
    "                     np.mean(shuffled_TEs[1,:]) )\n",
    "        \n",
    "        ## Return the self.DF value to the unshuffled case\n",
    "        return p_values, z_scores, TE_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7ff2af61-cedc-49b5-a7f2-4375541a0900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0] ((0.0, 0.0), (nan, nan), (0.0, 0.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-2504a8a586a0>:60: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z_scores = ( ( TE[0] - np.mean(shuffled_TEs[0,:]) ) / np.std(shuffled_TEs[0,:]) , \\\n",
      "<ipython-input-95-2504a8a586a0>:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ( TE[1] - np.mean(shuffled_TEs[1,:]) ) / np.std(shuffled_TEs[1,:])  )\n"
     ]
    }
   ],
   "source": [
    "endog = 'Anti-Regulation Fear-of-Regulation'\n",
    "exog = 'Daily Background Checks'\n",
    "te = non_linear_transfer_entropy(df, endog, exog)\n",
    "print(te, significance(df, te, endog, exog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e3e9917e-03f4-4809-ad84-117b4c9e0d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0] ((0.0, 0.0), (nan, nan), (0.0, 0.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-95-2504a8a586a0>:60: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  z_scores = ( ( TE[0] - np.mean(shuffled_TEs[0,:]) ) / np.std(shuffled_TEs[0,:]) , \\\n",
      "<ipython-input-95-2504a8a586a0>:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ( TE[1] - np.mean(shuffled_TEs[1,:]) ) / np.std(shuffled_TEs[1,:])  )\n"
     ]
    }
   ],
   "source": [
    "endog = 'Anti-Regulation Fear-of-Regulation'\n",
    "exog = 'Daily Background Checks'\n",
    "te = non_linear_transfer_entropy(df, endog, exog)\n",
    "print(te, significance(df, te, endog, exog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "53d1d9d0-6644-4bd5-bf87-ca33f61ef904",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Anti-Regulation', 'Pro-Regulation'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-5754fee6626f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mendog\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_linear_transfer_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0msignificance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-50157fab0c43>\u001b[0m in \u001b[0;36mnon_linear_transfer_entropy\u001b[0;34m(df, endog, exog, lags)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m#pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m#lag = 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendog\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma_bins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1550\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m         self._validate_read_indexer(\n\u001b[0m\u001b[1;32m   1553\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Anti-Regulation', 'Pro-Regulation'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "variables = [\n",
    "    'Pro-Regulation',\n",
    "    'Anti-Regulation',\n",
    "    'Self-Defense',\n",
    "    'Fear-of-Regulation',\n",
    "    'Pro-Regulation Self-Defense',\n",
    "    'Anti-Regulation Self-defense',\n",
    "    'Pro-Regulation Fear-of-Regulation',\n",
    "    'Anti-Regulation Fear-of-Regulation',\n",
    "    'Daily Background Checks'\n",
    "]\n",
    "\n",
    "for endog in variables:\n",
    "    for exog in variables:\n",
    "        if endog == exog:\n",
    "            continue\n",
    "        te = non_linear_transfer_entropy(df, endog, exog)\n",
    "        p, z, mean =  significance(df, te, endog, exog)\n",
    "        if p[0] < 0.1 or p[1] < 0.1:\n",
    "            print(f'X = {endog} Y = {exog}, p-values =  {p}, transfer entropy =  {te}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43c1ea9-108f-4c5b-b3e3-aa454e07124d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
