{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "445b607d-14a1-4f06-b0e4-86d0bb392ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma, atleast_2d, pi, sqrt, sum, transpose\n",
    "from scipy import stats, optimize, linalg, special\n",
    "from scipy.special import gammaln, logsumexp\n",
    "#from scipy._lib.six import callable, string_types\n",
    "from scipy.stats.mstats import mquantiles\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.cm as cm           \n",
    "\n",
    "import warnings, sys\n",
    "\n",
    "\n",
    "class NDHistogram():\n",
    "    \"\"\"\n",
    "        Custom histogram class wrapping the default numpy implementations (np.histogram, np.histogramdd). \n",
    "        This allows for dimension-agnostic histogram calculations, custom auto-binning and \n",
    "        associated data and methods to be stored for each object (e.g. Probability Density etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, bins=None, max_bins = 15):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            df          -   DataFrame passed through from the TransferEntropy class\n",
    "            bins        -   Bin edges passed through from the TransferEntropy class\n",
    "            max_bins    -   Number of bins per each dimension passed through from the TransferEntropy class\n",
    "        Returns:\n",
    "            self.pdf    -   This is an N-dimensional Probability Density Function, stored as a\n",
    "                            Numpy histogram, representing the proportion of samples in each bin.\n",
    "        \"\"\"\n",
    "        df = sanitise(df)\n",
    "        self.df = df.reindex(columns= sorted(df.columns))   # Sort axes by name\n",
    "        self.max_bins = max_bins\n",
    "        self.axes = list(self.df.columns.values)\n",
    "        self.bins = bins\n",
    "        self.n_dims = len(self.axes)\n",
    "        \n",
    "        ## Bins must match number and order of dimensions\n",
    "        if self.bins is None:\n",
    "            AB = AutoBins(self.df)\n",
    "            self.bins = AB.sigma_bins(max_bins=max_bins)\n",
    "        elif set(self.bins.keys()) != set(self.axes):\n",
    "            warnings.warn('Incompatible bins provided - defaulting to sigma bins')\n",
    "            AB = AutoBins(self.df)\n",
    "            self.bins = AB.sigma_bins(max_bins=max_bins)\n",
    "            \n",
    "        ordered_bins = [sorted(self.bins[key]) for key in sorted(self.bins.keys())]\n",
    "\n",
    "        ## Create ND histogram (np.histogramdd doesn't scale down to 1D)\n",
    "        if self.n_dims == 1:\n",
    "            self.Hist, self.Dedges = np.histogram(self.df.values,bins=ordered_bins[0], normed=False)\n",
    "        elif self.n_dims > 1:\n",
    "            self.Hist, self.Dedges = np.histogramdd(self.df.values,bins=ordered_bins, normed=False)\n",
    "        \n",
    "\n",
    "        ## Empirical Probability Density Function\n",
    "        if self.Hist.sum() == 0:   \n",
    "            print(self.Hist.shape)\n",
    "            \n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "                print(self.df.tail(40))\n",
    "\n",
    "            sys.exit(\"User-defined histogram is empty. Check bins or increase data points\")\n",
    "        else:\n",
    "            self.pdf = self.Hist/self.Hist.sum()\n",
    "            self._set_entropy_(self.pdf)\n",
    "  \n",
    "    def _set_entropy_(self,pdf):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            pdf   -   Probabiiity Density Function; this is calculated using the N-dimensional histogram above.\n",
    "        Returns:\n",
    "            n/a   \n",
    "        Sets entropy for marginal distributions: H(X), H(Y) etc. as well as joint entropy H(X,Y)\n",
    "        \"\"\"\n",
    "        ## Prepare empty dict for marginal entropies along each dimension\n",
    "        self.H = {}\n",
    "\n",
    "        if self.n_dims >1:\n",
    "            \n",
    "            ## Joint entropy H(X,Y) = -sum(pdf(x,y) * log(pdf(x,y)))     \n",
    "            self.H_joint =  -np.sum(pdf * ma.log2(pdf).filled(0)) # Use masking to replace log(0) with 0\n",
    "\n",
    "            ## Single entropy for each dimension H(X) = -sum(pdf(x) * log(pdf(x)))\n",
    "            for a, axis_name in enumerate(self.axes):\n",
    "                self.H[axis_name] =  -np.sum(pdf.sum(axis=a) * ma.log2(pdf.sum(axis=a)).filled(0)) # Use masking to replace log(0) with 0\n",
    "        else:\n",
    "            ## Joint entropy and single entropy are the same\n",
    "            self.H_joint = -np.sum(pdf * ma.log2(pdf).filled(0)) \n",
    "            self.H[self.df.columns[0]] = self.H_joint\n",
    "\n",
    "\n",
    "class LaggedTimeSeries():\n",
    "    \"\"\"\n",
    "        Custom wrapper class for pandas DataFrames for performing predictive analysis.\n",
    "        Generates lagged time series and performs custom windowing over datetime indexes\n",
    "    \"\"\"\n",
    "    def __init__(self, df, lag=None, max_lag_only=True, window_size = None, window_stride = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df              -   Pandas DataFrame object of N columns. Must be indexed as an increasing \n",
    "                                time series (i.e. past-to-future), with equal timesteps between each row\n",
    "            lags            -   The number of steps to be included. Each increase in Lags will result \n",
    "                                in N additional columns, where N is the number of columns in the original \n",
    "                                dataframe. It will also remove the first N rows.\n",
    "            max_lag_only    -   Defines whether the returned dataframe contains all lagged timeseries up to \n",
    "                                and including the defined lag, or only the time series equal to this lag value\n",
    "            window_size     -   Dict containing key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                Describes the desired size of each window, provided the data is indexed with datetime type. Leave as\n",
    "                                None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "            window_stride   -   Dict containing key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                Describes the size of the step between consecutive windows, provided the data is indexed with datetime type. Leave as\n",
    "                                None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "                       \n",
    "        Returns:    -   n/a\n",
    "        \"\"\"        \n",
    "        self.df = sanitise(df)\n",
    "        self.axes = list(self.df.columns.values) #Variable names\n",
    "\n",
    "        self.max_lag_only = max_lag_only\n",
    "        if lag is not None:\n",
    "            self.t = lag\n",
    "            self.df = self.__apply_lags__()\n",
    "\n",
    "        if window_size is not None and window_stride is not None:\n",
    "            self.has_windows = True\n",
    "            self. __apply_windows__(window_size, window_stride)\n",
    "        else:\n",
    "            self.has_windows = False\n",
    "\n",
    "    def __apply_lags__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n/a\n",
    "        Returns:\n",
    "            new_df.iloc[self.t:]    -   This is a new dataframe containing the original columns and\n",
    "                                        all lagged columns. Note that the first few rows (equal to self.lag) will\n",
    "                                        be removed from the top, since lagged values are of coursenot available\n",
    "                                        for these indexes.\n",
    "        \"\"\"\n",
    "        # Create a new dataframe to maintain the new data, dropping rows with NaN\n",
    "        new_df = self.df.copy() #.dropna()\n",
    "\n",
    "        # Create new column with lagged timeseries for each variable\n",
    "        col_names = self.df.columns.values.tolist()\n",
    "\n",
    "        # If the user wants to only consider the time series lagged by the \n",
    "        # maximum number specified or by every series up to an including the maximum lag:\n",
    "        if self.max_lag_only == True:\n",
    "            for col_name in col_names:\n",
    "                new_df[col_name + '_lag' + str(self.t)] = self.df[col_name].shift(self.t)\n",
    "\n",
    "        elif self.max_lag_only == False:\n",
    "            for col_name in col_names:\n",
    "                for t in range(1,self.t+1):\n",
    "                    new_df[col_name + '_lag' + str(t)] = self.df[col_name].shift(t)\n",
    "        else:\n",
    "            raise ValueError('Error')\n",
    "\n",
    "        # Drop the first t rows, which now contain NaN\n",
    "        return new_df.iloc[self.t:]\n",
    "\n",
    "    def __apply_windows__(self, window_size, window_stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            window_size      -   Dict passed from self.__init__\n",
    "            window_stride    -   Dict passed from self.__init__\n",
    "        Returns:    \n",
    "            n/a              -   Sets the daterange for the self.windows property to iterate along\n",
    "        \"\"\"\n",
    "        self.window_size =  {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "        self.window_stride =  {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "\n",
    "        self.window_stride.update(window_stride)\n",
    "        self.window_size.update(window_size)\n",
    "        freq = ''\n",
    "        daterangefreq = freq.join([str(v)+str(k) for (k,v) in self.window_stride.items() if v != 0])\n",
    "        self.daterange = pd.date_range(self.df.index.min(),self.df.index.max() , freq=daterangefreq)\n",
    "\n",
    "    def date_diff(self,window_size):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            window_size     -    Dict passed from self.windows function\n",
    "        Returns:\n",
    "            start_date      -    The start date of the proposed window\n",
    "            end_date        -    The end date of the proposed window    \n",
    "        \n",
    "        This function is TBC - proposed due to possible duplication of the relativedelta usage in self.windows and self.headstart\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def windows(self):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            n/a\n",
    "        Returns:\n",
    "            windows         -   Generator defining a pandas DataFrame for each window of the data. \n",
    "                                Usage like:   [window for window in LaggedTimeSeries.windows]\n",
    "        \"\"\"\n",
    "        if self.has_windows == False:\n",
    "            return self.df\n",
    "        ## Loop Over TimeSeries Range\n",
    "        for i,dt in enumerate(self.daterange):\n",
    "            \n",
    "            ## Ensure Each Division Contains Required Number of Months\n",
    "            if dt-relativedelta(years   =  self.window_size['YS'],\n",
    "                                months  =  self.window_size['MS'],\n",
    "                                days    =  self.window_size['D'],\n",
    "                                hours   =  self.window_size['H'],\n",
    "                                minutes =  self.window_size['min'],\n",
    "                                seconds =  self.window_size['S'],\n",
    "                                microseconds = self.window_size['ms']\n",
    "                                ) >= self.df.index.min():\n",
    "                \n",
    "                ## Create Window \n",
    "                yield self.df.loc[(dt-relativedelta(years   =  self.window_size['YS'],\n",
    "                                                    months  =  self.window_size['MS'],\n",
    "                                                    days    =  self.window_size['D'],\n",
    "                                                    hours   =  self.window_size['H'],\n",
    "                                                    minutes =  self.window_size['min'],\n",
    "                                                    seconds =  self.window_size['S'],\n",
    "                                                    microseconds = self.window_size['ms']\n",
    "                                                    )) : dt]\n",
    "\n",
    "    @property\n",
    "    def headstart(self):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            n/a\n",
    "        Returns:\n",
    "            len(windows)    -   The number of windows which would have start dates before the desired date range. \n",
    "                                Used in TransferEntropy class to slice off incomplete windows.\n",
    "            \n",
    "        \"\"\"\n",
    "        windows =   [i for i,dt in enumerate(self.daterange) \n",
    "                            if dt-relativedelta(    years   =  self.window_size['YS'],\n",
    "                                                    months  =  self.window_size['MS'],\n",
    "                                                    days    =  self.window_size['D'],\n",
    "                                                    hours   =  self.window_size['H'],\n",
    "                                                    minutes =  self.window_size['min'],\n",
    "                                                    seconds =  self.window_size['S'],\n",
    "                                                    microseconds = self.window_size['ms']\n",
    "                                        ) < self.df.index.min() ]\n",
    "        ## i.e. count from the first window which falls entirely after the earliest date\n",
    "        return len(windows)\n",
    "\n",
    "\n",
    "class TransferEntropy():\n",
    "    \"\"\"\n",
    "        Functional class to calculate Transfer Entropy between time series, to detect causal signals.\n",
    "        Currently accepts two series: X(t) and Y(t). Future extensions planned to accept additional endogenous \n",
    "        series: X1(t), X2(t), X3(t) etc. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, DF, endog, exog, lag = None, window_size=None, window_stride=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            DF            -   (DataFrame) Time series data for X and Y (NOT including lagged variables)\n",
    "            endog         -   (string)    Fieldname for endogenous (dependent) variable Y\n",
    "            exog          -   (string)    Fieldname for exogenous (independent) variable X\n",
    "            lag           -   (integer)   Number of periods (rows) by which to lag timeseries data\n",
    "            window_size   -   (Dict)      Must contain key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                          Describes the desired size of each window, provided the data is indexed with datetime type. Leave as\n",
    "                                          None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "            window_stride -   (Dict)      Must contain key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                          Describes the size of the step between consecutive windows, provided the data is indexed with datetime type. Leave as\n",
    "                                          None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "        Returns:\n",
    "            n/a\n",
    "        \"\"\"\n",
    "        self.lts = LaggedTimeSeries(df=sanitise(DF), \n",
    "                                    lag=lag, \n",
    "                                    window_size=window_size,\n",
    "                                    window_stride=window_stride)\n",
    "\n",
    "        if self.lts.has_windows is True:\n",
    "            self.df = self.lts.windows\n",
    "            self.date_index = self.lts.daterange[self.lts.headstart:]\n",
    "            self.results = pd.DataFrame(index=self.date_index)\n",
    "            self.results.index.name = \"windows_ending_on\"\n",
    "        else:\n",
    "            self.df = [self.lts.df]\n",
    "            self.results = pd.DataFrame(index=[0])\n",
    "        self.max_lag_only = True\n",
    "        self.endog = endog                             # Dependent Variable Y\n",
    "        self.exog = exog                               # Independent Variable X\n",
    "        self.lag = lag\n",
    "\n",
    "        \n",
    "        \"\"\" If using KDE, this ensures the covariance matrices are calculated once over all data, rather\n",
    "            than for each window. This saves computational time and provides a fair point for comparison.\"\"\"\n",
    "        self.covars = [[],[]]\n",
    "        for i,(X,Y) in enumerate({self.exog:self.endog, self.endog:self.exog}.items()):\n",
    "            X_lagged = X+'_lag'+str(self.lag)\n",
    "            Y_lagged = Y+'_lag'+str(self.lag)\n",
    "\n",
    "            self.covars[i] = [  np.cov(self.lts.df[[Y,Y_lagged,X_lagged]].values.T),\n",
    "                                np.cov(self.lts.df[[X_lagged,Y_lagged]].values.T),\n",
    "                                np.cov(self.lts.df[[Y,Y_lagged]].values.T),\n",
    "                                np.ones(shape=(1,1)) * self.lts.df[Y_lagged].std()**2 ]\n",
    "\n",
    "   \n",
    "    def nonlinear_TE(self, df=None, pdf_estimator='histogram', bins=None, bandwidth=None, gridpoints=20, n_shuffles=0):\n",
    "        \"\"\"\n",
    "        NonLinear Transfer Entropy for directional causal inference\n",
    "        Defined:            TE = TE_XY - TE_YX      where TE_XY = H(Y|Y-t) - H(Y|Y-t,X-t)\n",
    "        Calculated using:   H(Y|Y-t,X-t) = H(Y,Y-t,X-t) - H(Y,Y-t)  and finding joint entropy through density estimation\n",
    "        Arguments:\n",
    "            pdf_estimator   -   (string)    'Histogram' or 'kernel' Used to define which method is preferred for density estimation\n",
    "                                            of the distribution - either histogram or KDE\n",
    "            bins            -   (dict of lists) Optional parameter to provide hard-coded bin-edges. Dict keys \n",
    "                                            must contain names of variables - including lagged columns! Dict values must be lists\n",
    "                                            containing bin-edge numerical values. \n",
    "            bandwidth       -   (float)     Optional parameter for custom bandwidth in KDE. This is a scalar multiplier to the covariance\n",
    "                                            matrix used (see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.covariance_factor.html)\n",
    "            gridpoints      -   (integer)   Number of gridpoints (in each dimension) to discretise the probablity space when performing\n",
    "                                            integration of the kernel density estimate. Increasing this gives more precision, but significantly\n",
    "                                            increases execution time\n",
    "            n_shuffles      -   (integer)   Number of times to shuffle the dataframe, destroying the time series temporality, in order to \n",
    "                                            perform significance testing.\n",
    "        Returns:\n",
    "            transfer_entropies  -  (list) Directional Transfer Entropies from X(t)->Y(t) and Y(t)->X(t) respectively\n",
    "        \n",
    "        (Also stores TE, Z-score and p-values in self.results - for each window if windows defined.)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        ## Retrieve user-defined bins\n",
    "        self.bins = bins\n",
    "        if self.bins is None:\n",
    "            self.bins = {self.endog: None}\n",
    "\n",
    "        ## Prepare lists for storing results\n",
    "        TEs = []\n",
    "        shuffled_TEs = []\n",
    "        p_values = []\n",
    "        z_scores = []\n",
    "\n",
    "        ## Loop over all windows\n",
    "        for i,df in enumerate(self.df):\n",
    "            df = deepcopy(df)\n",
    "\n",
    "            ## Shows user that something is happening\n",
    "            if self.lts.has_windows is True:\n",
    "                print(\"Window ending: \", self.date_index[i])\n",
    "\n",
    "            ## Initialise list to return TEs\n",
    "            transfer_entropies = [0,0]\n",
    "\n",
    "            ## Require us to compare information transfer bidirectionally\n",
    "            for i,(X,Y) in enumerate({self.exog:self.endog, self.endog:self.exog}.items()):\n",
    "                \n",
    "                ### Entropy calculated using Probability Density Estimation:\n",
    "                    # Following: https://stat.ethz.ch/education/semesters/SS_2006/CompStat/sk-ch2.pdf\n",
    "                    # Also: https://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs/lec5.pdf\n",
    "                \n",
    "                ## Note Lagged Terms\n",
    "                X_lagged = X+'_lag'+str(self.lag)\n",
    "                Y_lagged = Y+'_lag'+str(self.lag)\n",
    "\n",
    "                ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "\n",
    "                ## 1. H(Y,Y-t,X-t)  \n",
    "                H1 = get_entropy(df = df[[Y,Y_lagged,X_lagged]], \n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth = bandwidth, \n",
    "                                estimator = pdf_estimator,\n",
    "                                bins = {k:v for (k,v) in self.bins.items()\n",
    "                                        if k in[Y,Y_lagged,X_lagged]},\n",
    "                                covar = self.covars[i][0])\n",
    "                ## 2. H(Y-t,X-t)\n",
    "                H2 = get_entropy(df = df[[X_lagged,Y_lagged]],\n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth = bandwidth,\n",
    "                                estimator = pdf_estimator,\n",
    "                                bins = {k:v for (k,v) in self.bins.items() \n",
    "                                        if k in [X_lagged,Y_lagged]},\n",
    "                                covar = self.covars[i][1]) \n",
    "                ## 3. H(Y,Y-t)  \n",
    "                H3 = get_entropy(df = df[[Y,Y_lagged]],\n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth  = bandwidth,\n",
    "                                estimator = pdf_estimator,\n",
    "                                bins =  {k:v for (k,v) in self.bins.items() \n",
    "                                        if k in [Y,Y_lagged]},\n",
    "                                covar = self.covars[i][2])\n",
    "                ## 4. H(Y-t)  \n",
    "                H4 = get_entropy(df = df[[Y_lagged]],\n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth  = bandwidth,\n",
    "                                estimator = pdf_estimator,\n",
    "                                bins =  {k:v for (k,v) in self.bins.items() \n",
    "                                        if k in [Y_lagged]},\n",
    "                                covar = self.covars[i][3])                \n",
    "\n",
    "\n",
    "                ### Calculate Conditonal Entropy using: H(Y|X-t,Y-t) = H(Y,X-t,Y-t) - H(X-t,Y-t)\n",
    "                conditional_entropy_joint =  H1 - H2\n",
    "            \n",
    "                ### And Conditional Entropy independent of X(t) H(Y|Y-t) = H(Y,Y-t) - H(Y-t)            \n",
    "                conditional_entropy_independent = H3 - H4\n",
    "\n",
    "                ### Directional Transfer Entropy is the difference between the conditional entropies\n",
    "                transfer_entropies[i] =  conditional_entropy_independent - conditional_entropy_joint\n",
    "            \n",
    "            TEs.append(transfer_entropies)\n",
    "\n",
    "            ## Calculate Significance of TE during this window\n",
    "            if n_shuffles > 0:\n",
    "                p, z, TE_mean = significance(    df = df, \n",
    "                                        TE = transfer_entropies, \n",
    "                                        endog = self.endog, \n",
    "                                        exog = self.exog, \n",
    "                                        lag = self.lag, \n",
    "                                        n_shuffles = n_shuffles, \n",
    "                                        pdf_estimator = pdf_estimator, \n",
    "                                        bins = self.bins,\n",
    "                                        bandwidth = bandwidth,\n",
    "                                        method='nonlinear')\n",
    "\n",
    "                shuffled_TEs.append(TE_mean)\n",
    "                p_values.append(p)\n",
    "                z_scores.append(z)\n",
    "\n",
    "        ## Store Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "        self.add_results({'TE_XY' : np.array(TEs)[:,0],\n",
    "                          'TE_YX' : np.array(TEs)[:,1],\n",
    "                          'p_value_XY' : None,\n",
    "                          'p_value_YX' : None,\n",
    "                          'z_score_XY' : 0,\n",
    "                          'z_score_YX' : 0\n",
    "                          })\n",
    "        if n_shuffles > 0:\n",
    "            ## Store Significance Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "            \n",
    "            self.add_results({'p_value_XY' : np.array(p_values)[:,0],\n",
    "                              'p_value_YX' : np.array(p_values)[:,1],\n",
    "                              'z_score_XY' : np.array(z_scores)[:,0],\n",
    "                              'z_score_YX' : np.array(z_scores)[:,1],\n",
    "                              'Ave_TE_XY'  : np.array(shuffled_TEs)[:,0],\n",
    "                              'Ave_TE_YX'  : np.array(shuffled_TEs)[:,1]\n",
    "                            })\n",
    "\n",
    "        return transfer_entropies\n",
    "\n",
    "    def add_results(self,dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dict    -   JSON-style data to store in existing self.results DataFrame\n",
    "        Returns:\n",
    "            n/a\n",
    "        \"\"\"\n",
    "        for (k,v) in dict.items():\n",
    "            self.results[str(k)] = v\n",
    "            \n",
    "def significance(df, TE, endog, exog, lag, n_shuffles, method, pdf_estimator=None, bins=None, bandwidth=None,  both=True):\n",
    "        \"\"\"\n",
    "        Perform significance analysis on the hypothesis test of statistical causality, for both X(t)->Y(t)\n",
    "        and Y(t)->X(t) directions\n",
    "   \n",
    "        Calculated using:  Assuming stationarity, we shuffle the time series to provide the null hypothesis. \n",
    "                           The proportion of tests where TE > TE_shuffled gives the p-value significance level.\n",
    "                           The amount by which the calculated TE is greater than the average shuffled TE, divided\n",
    "                           by the standard deviation of the results, is the z-score significance level.\n",
    "        Arguments:\n",
    "            TE              -      (list)    Contains the transfer entropy in each direction, i.e. [TE_XY, TE_YX]\n",
    "            endog           -      (string)  The endogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            exog            -      (string)  The exogenous variable in the TE analysis being significance tested (i.e. X or Y) \n",
    "            pdf_estimator   -      (string)  The pdf_estimator used in the original TE analysis\n",
    "            bins            -      (Dict of lists)  The bins used in the original TE analysis\n",
    "            n_shuffles      -      (float) Number of times to shuffle the dataframe, destroyig temporality\n",
    "            both            -      (Bool) Whether to shuffle both endog and exog variables (z-score) or just exog                                  variables (giving z*-score)  \n",
    "        Returns:\n",
    "            p_value         -      Probablity of observing the result given the null hypothesis\n",
    "            z_score         -      Number of Standard Deviations result is from mean (normalised)\n",
    "        \"\"\" \n",
    "\n",
    "        ## Prepare array for Transfer Entropy of each Shuffle\n",
    "        shuffled_TEs = np.zeros(shape = (2,n_shuffles))\n",
    "        \n",
    "        ##\n",
    "        if both is True:\n",
    "            pass #TBC\n",
    "\n",
    "        for i in range(n_shuffles):\n",
    "                ## Perform Shuffle\n",
    "                df = shuffle_series(df)\n",
    "                \n",
    "                ## Calculate New TE\n",
    "                shuffled_causality = TransferEntropy(   DF = df,\n",
    "                                                endog = endog,     \n",
    "                                                exog = exog,          \n",
    "                                                lag = lag\n",
    "                                            )    \n",
    "                if method == 'linear':\n",
    "                    TE_shuffled = shuffled_causality.linear_TE(df, n_shuffles=0)\n",
    "                else:       \n",
    "                    TE_shuffled = shuffled_causality.nonlinear_TE(df, pdf_estimator, bins, bandwidth, n_shuffles=0)\n",
    "                shuffled_TEs[:,i] = TE_shuffled\n",
    "\n",
    "        \n",
    "        ## Calculate p-values for each direction\n",
    "        p_values = (np.count_nonzero(TE[0] < shuffled_TEs[0,:]) / n_shuffles, \\\n",
    "                    np.count_nonzero(TE[1] < shuffled_TEs[1,:]) /n_shuffles)\n",
    "\n",
    "        ## Calculate z-scores for each direction\n",
    "        z_scores = ( ( TE[0] - np.mean(shuffled_TEs[0,:]) ) / np.std(shuffled_TEs[0,:]) , \\\n",
    "                     ( TE[1] - np.mean(shuffled_TEs[1,:]) ) / np.std(shuffled_TEs[1,:])  )\n",
    "        \n",
    "        TE_mean = ( np.mean(shuffled_TEs[0,:]), \\\n",
    "                     np.mean(shuffled_TEs[1,:]) )\n",
    "        \n",
    "        ## Return the self.DF value to the unshuffled case\n",
    "        return p_values, z_scores, TE_mean\n",
    "\n",
    "def get_entropy(df, gridpoints=15, bandwidth=None, estimator='kernel', bins=None, covar=None):\n",
    "    \"\"\"\n",
    "        Function for calculating entropy from a probability mass \n",
    "        \n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                        the domain. Used if estimator='kernel'\n",
    "        bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                        matrix). Used if estimator='kernel'\n",
    "        estimator   -       (string)    'histogram' or 'kernel'\n",
    "        bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator\n",
    "                                        = 'histogram'\n",
    "        covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                        Used if estimator = 'kernel'\n",
    "    Returns:\n",
    "        entropy     -       (float)     Shannon entropy in bits\n",
    "    \"\"\"\n",
    "    pdf = get_pdf(df, gridpoints, bandwidth, estimator, bins, covar)\n",
    "    ## log base 2 returns H(X) in bits\n",
    "    return -np.sum( pdf * ma.log2(pdf).filled(0)) \n",
    "\n",
    "def shuffle_series(DF, only=None):\n",
    "    \"\"\"\n",
    "    Function to return time series shuffled rowwise along each desired column. \n",
    "    Each column is shuffled independently, removing the temporal relationship.\n",
    "    This is to calculate Z-score and Z*-score. See P. Boba et al (2015)\n",
    "    Calculated using:       df.apply(np.random.permutation)\n",
    "    Arguments:\n",
    "        df              -   (DataFrame) Time series data \n",
    "        only            -   (list)      Fieldnames to shuffle. If none, all columns shuffled \n",
    "    Returns:\n",
    "        df_shuffled     -   (DataFrame) Time series shuffled along desired columns    \n",
    "    \"\"\"\n",
    "    if not only == None:\n",
    "        shuffled_DF = DF.copy()\n",
    "        for col in only:\n",
    "            series = DF.loc[:, col].to_frame()\n",
    "            shuffled_DF[col] = series.apply(np.random.permutation)\n",
    "    else:\n",
    "        shuffled_DF = DF.apply(np.random.permutation)\n",
    "    \n",
    "    return shuffled_DF\n",
    "\n",
    "def get_pdf(df, gridpoints=None, bandwidth=None, estimator=None, bins=None, covar=None):\n",
    "    \"\"\"\n",
    "        Function for non-parametric density estimation\n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                        the domain. Used if estimator='kernel'\n",
    "        bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                        matrix). Used if estimator='kernel'\n",
    "        estimator   -       (string)    'histogram' or 'kernel'\n",
    "        bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator = 'histogram'\n",
    "        covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                        Used if estimator = 'kernel'\n",
    "    Returns:\n",
    "        pdf         -       (Numpy ndarray) Probability of a sample being in a specific \n",
    "                                        bin (technically a probability mass)\n",
    "    \"\"\"\n",
    "    DF = sanitise(df)\n",
    "    \n",
    "    if estimator == 'histogram':\n",
    "        pdf = pdf_histogram(DF, bins)\n",
    "    else:\n",
    "        pdf = pdf_kde(DF, gridpoints, bandwidth, covar)\n",
    "    return pdf\n",
    "\n",
    "def pdf_kde(df, gridpoints=None, bandwidth=1, covar=None):\n",
    "    \"\"\"\n",
    "        Function for non-parametric density estimation using Kernel Density Estimation\n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                        the domain. Used if estimator='kernel'\n",
    "        bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                        matrix).\n",
    "        covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                        If None, these are calculated from df during the \n",
    "                                        KDE analysis\n",
    "    Returns:\n",
    "        Z/Z.sum()   -       (Numpy ndarray) Probability of a sample being between\n",
    "                                        specific gridpoints (technically a probability mass)\n",
    "    \"\"\"\n",
    "    ## Create Meshgrid to capture data\n",
    "    if gridpoints is None:\n",
    "        gridpoints = 20\n",
    "    \n",
    "    N = complex(gridpoints)\n",
    "    \n",
    "    slices = [slice(dim.min(),dim.max(),N) for dimname, dim in df.iteritems()]\n",
    "    grids = np.mgrid[slices]\n",
    "\n",
    "    ## Pass Meshgrid to Scipy Gaussian KDE to Estimate PDF\n",
    "    positions = np.vstack([X.ravel() for X in grids])\n",
    "    values = df.values.T\n",
    "    kernel = _kde_(values, bw_method=bandwidth, covar=covar)\n",
    "    Z = np.reshape(kernel(positions).T, grids[0].shape) \n",
    "\n",
    "    ## Normalise \n",
    "    return Z/Z.sum()\n",
    "\n",
    "def pdf_histogram(df,bins):\n",
    "    \"\"\"\n",
    "        Function for non-parametric density estimation using N-Dimensional Histograms\n",
    "    Args:\n",
    "        df            -       (DataFrame) Samples over which to estimate density\n",
    "        bins          -       (Dict of lists) Bin edges for NDHistogram. \n",
    "    Returns:\n",
    "        histogram.pdf -       (Numpy ndarray) Probability of a sample being in a specific \n",
    "                                    bin (technically a probability mass)\n",
    "    \"\"\"\n",
    "    histogram = NDHistogram(df=df, bins=bins)        \n",
    "    return histogram.pdf\n",
    "\n",
    "def plot_pdf_histogram(df,bins, cmap='inferno'):\n",
    "    \"\"\"\n",
    "    Function to plot the pdf of a dataset, estimated via histogram.\n",
    "        \n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator = 'histogram'\n",
    "    Returns:\n",
    "        ax          -       AxesSubplot object, passed back via to plot_pdf() function\n",
    "    \"\"\"\n",
    "    DF = sanitise(df) # in case function called directly\n",
    "\n",
    "\n",
    "    ## Calculate PDF\n",
    "    PDF = get_pdf(df=DF,estimator='histogram',bins=bins)\n",
    "\n",
    "    ## Get x-coords, y-coords for each bar\n",
    "    (x_edges,y_edges) = bins.values()\n",
    "    X, Y = np.meshgrid(x_edges[:-1], y_edges[:-1])\n",
    "    ## Get dx, dy for each bar\n",
    "    dxs, dys = np.meshgrid(np.diff(x_edges),np.diff(y_edges))\n",
    "\n",
    "    ## Colourmap\n",
    "    cmap = cm.get_cmap(cmap) \n",
    "    rgba = [cmap((p-PDF.flatten().min())/PDF.flatten().max()) for p in PDF.flatten()] \n",
    "\n",
    "    ## Create subplots\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.bar3d(   x = X.flatten(),            #x coordinates of each bar\n",
    "                y = Y.flatten(),            #y coordinates of each bar\n",
    "                z = 0,                      #z coordinates of each bar\n",
    "                dx = dxs.flatten(),         #width of each bar\n",
    "                dy = dys.flatten(),         #depth of each bar\n",
    "                dz = PDF.flatten() ,        #height of each bar\n",
    "                alpha = 1,                  #transparency\n",
    "                color = rgba\n",
    "    )\n",
    "    ax.set_title(\"Histogram Probability Distribution\",fontsize=10)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def plot_pdf_kernel(df,gridpoints=None, bandwidth=None, covar=None, cmap='inferno'):\n",
    "    \"\"\"\n",
    "        Function to plot the pdf, calculated by KDE, of a dataset\n",
    "        \n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                        the domain. Used if estimator='kernel'\n",
    "        bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                        matrix). Used if estimator='kernel'\n",
    "        covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                        \n",
    "    Returns:\n",
    "        ax          -       AxesSubplot object, passed back via to plot_pdf() function\n",
    "    \"\"\"\n",
    "    DF = sanitise(df)\n",
    "    ## Estimate the PDF from the data\n",
    "    if gridpoints is None:\n",
    "        gridpoints = 20\n",
    "\n",
    "    pdf = get_pdf(DF,gridpoints=gridpoints,bandwidth=bandwidth)    \n",
    "    N = complex(gridpoints) \n",
    "    slices = [slice(dim.min(),dim.max(),N) for dimname, dim in DF.iteritems()]\n",
    "    X,Y = np.mgrid[slices]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X, Y, pdf, cmap=cmap)\n",
    "    \n",
    "\n",
    "    ax.set_title(\"KDE Probability Distribution\",fontsize=10)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "def sanitise(df):\n",
    "    \"\"\"\n",
    "        Function to convert DataFrame-like objects into pandas DataFrames\n",
    "        \n",
    "    Args:\n",
    "        df          -        Data in pd.Series or pd.DataFrame format\n",
    "    Returns:\n",
    "        df          -        Data as pandas DataFrame\n",
    "    \"\"\"\n",
    "    ## Ensure data is in DataFrame form\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        df = df\n",
    "    elif isinstance(df, pd.Series):\n",
    "        df = df.to_frame()\n",
    "    else:\n",
    "        raise ValueError('Data passed as %s Please ensure your data is stored as a Pandas DataFrame' %(str(type(df))) )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6dd66afb-fb9f-481b-a256-a7474a9defe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma, atleast_2d, pi, sqrt, sum, transpose\n",
    "from scipy import stats, optimize, linalg, special\n",
    "from scipy.special import gammaln, logsumexp\n",
    "#from scipy._lib.six import callable, string_types\n",
    "from scipy.stats.mstats import mquantiles\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.cm as cm           \n",
    "\n",
    "import warnings, sys\n",
    "\n",
    "\n",
    "class NDHistogram():\n",
    "    \"\"\"\n",
    "        Custom histogram class wrapping the default numpy implementations (np.histogram, np.histogramdd). \n",
    "        This allows for dimension-agnostic histogram calculations, custom auto-binning and \n",
    "        associated data and methods to be stored for each object (e.g. Probability Density etc.)\n",
    "    \"\"\"\n",
    "    def __init__(self, df, bins=None, max_bins = 15):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            df          -   DataFrame passed through from the TransferEntropy class\n",
    "            bins        -   Bin edges passed through from the TransferEntropy class\n",
    "            max_bins    -   Number of bins per each dimension passed through from the TransferEntropy class\n",
    "        Returns:\n",
    "            self.pdf    -   This is an N-dimensional Probability Density Function, stored as a\n",
    "                            Numpy histogram, representing the proportion of samples in each bin.\n",
    "        \"\"\"\n",
    "        df = sanitise(df)\n",
    "        self.df = df.reindex(columns= sorted(df.columns))   # Sort axes by name\n",
    "        self.max_bins = max_bins\n",
    "        self.axes = list(self.df.columns.values)\n",
    "        self.bins = bins\n",
    "        self.n_dims = len(self.axes)\n",
    "        \n",
    "        ## Bins must match number and order of dimensions\n",
    "        if self.bins is None:\n",
    "            AB = AutoBins(self.df)\n",
    "            self.bins = AB.sigma_bins(max_bins=max_bins)\n",
    "        elif set(self.bins.keys()) != set(self.axes):\n",
    "            warnings.warn('Incompatible bins provided - defaulting to sigma bins')\n",
    "            AB = AutoBins(self.df)\n",
    "            self.bins = AB.sigma_bins(max_bins=max_bins)\n",
    "            \n",
    "        ordered_bins = [sorted(self.bins[key]) for key in sorted(self.bins.keys())]\n",
    "\n",
    "        ## Create ND histogram (np.histogramdd doesn't scale down to 1D)\n",
    "        if self.n_dims == 1:\n",
    "            self.Hist, self.Dedges = np.histogram(self.df.values,bins=ordered_bins[0], normed=False)\n",
    "        elif self.n_dims > 1:\n",
    "            self.Hist, self.Dedges = np.histogramdd(self.df.values,bins=ordered_bins, normed=False)\n",
    "        \n",
    "\n",
    "        ## Empirical Probability Density Function\n",
    "        if self.Hist.sum() == 0:   \n",
    "            print(self.Hist.shape)\n",
    "            \n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "                print(self.df.tail(40))\n",
    "\n",
    "            sys.exit(\"User-defined histogram is empty. Check bins or increase data points\")\n",
    "        else:\n",
    "            self.pdf = self.Hist/self.Hist.sum()\n",
    "            self._set_entropy_(self.pdf)\n",
    "  \n",
    "    def _set_entropy_(self,pdf):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            pdf   -   Probabiiity Density Function; this is calculated using the N-dimensional histogram above.\n",
    "        Returns:\n",
    "            n/a   \n",
    "        Sets entropy for marginal distributions: H(X), H(Y) etc. as well as joint entropy H(X,Y)\n",
    "        \"\"\"\n",
    "        ## Prepare empty dict for marginal entropies along each dimension\n",
    "        self.H = {}\n",
    "\n",
    "        if self.n_dims >1:\n",
    "            \n",
    "            ## Joint entropy H(X,Y) = -sum(pdf(x,y) * log(pdf(x,y)))     \n",
    "            self.H_joint =  -np.sum(pdf * ma.log2(pdf).filled(0)) # Use masking to replace log(0) with 0\n",
    "\n",
    "            ## Single entropy for each dimension H(X) = -sum(pdf(x) * log(pdf(x)))\n",
    "            for a, axis_name in enumerate(self.axes):\n",
    "                self.H[axis_name] =  -np.sum(pdf.sum(axis=a) * ma.log2(pdf.sum(axis=a)).filled(0)) # Use masking to replace log(0) with 0\n",
    "        else:\n",
    "            ## Joint entropy and single entropy are the same\n",
    "            self.H_joint = -np.sum(pdf * ma.log2(pdf).filled(0)) \n",
    "            self.H[self.df.columns[0]] = self.H_joint\n",
    "\n",
    "\n",
    "class LaggedTimeSeries():\n",
    "    \"\"\"\n",
    "        Custom wrapper class for pandas DataFrames for performing predictive analysis.\n",
    "        Generates lagged time series and performs custom windowing over datetime indexes\n",
    "    \"\"\"\n",
    "    def __init__(self, df, lag=None, max_lag_only=True, window_size = None, window_stride = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df              -   Pandas DataFrame object of N columns. Must be indexed as an increasing \n",
    "                                time series (i.e. past-to-future), with equal timesteps between each row\n",
    "            lags            -   The number of steps to be included. Each increase in Lags will result \n",
    "                                in N additional columns, where N is the number of columns in the original \n",
    "                                dataframe. It will also remove the first N rows.\n",
    "            max_lag_only    -   Defines whether the returned dataframe contains all lagged timeseries up to \n",
    "                                and including the defined lag, or only the time series equal to this lag value\n",
    "            window_size     -   Dict containing key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                Describes the desired size of each window, provided the data is indexed with datetime type. Leave as\n",
    "                                None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "            window_stride   -   Dict containing key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                Describes the size of the step between consecutive windows, provided the data is indexed with datetime type. Leave as\n",
    "                                None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "                       \n",
    "        Returns:    -   n/a\n",
    "        \"\"\"        \n",
    "        self.df = sanitise(df)\n",
    "        self.axes = list(self.df.columns.values) #Variable names\n",
    "\n",
    "        self.max_lag_only = max_lag_only\n",
    "        if lag is not None:\n",
    "            self.t = lag\n",
    "            self.df = self.__apply_lags__()\n",
    "\n",
    "        if window_size is not None and window_stride is not None:\n",
    "            self.has_windows = True\n",
    "            self. __apply_windows__(window_size, window_stride)\n",
    "        else:\n",
    "            self.has_windows = False\n",
    "\n",
    "    def __apply_lags__(self):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n/a\n",
    "        Returns:\n",
    "            new_df.iloc[self.t:]    -   This is a new dataframe containing the original columns and\n",
    "                                        all lagged columns. Note that the first few rows (equal to self.lag) will\n",
    "                                        be removed from the top, since lagged values are of coursenot available\n",
    "                                        for these indexes.\n",
    "        \"\"\"\n",
    "        # Create a new dataframe to maintain the new data, dropping rows with NaN\n",
    "        new_df = self.df.copy() #.dropna()\n",
    "\n",
    "        # Create new column with lagged timeseries for each variable\n",
    "        col_names = self.df.columns.values.tolist()\n",
    "\n",
    "        # If the user wants to only consider the time series lagged by the \n",
    "        # maximum number specified or by every series up to an including the maximum lag:\n",
    "        if self.max_lag_only == True:\n",
    "            for col_name in col_names:\n",
    "                new_df[col_name + '_lag' + str(self.t)] = self.df[col_name].shift(self.t)\n",
    "\n",
    "        elif self.max_lag_only == False:\n",
    "            for col_name in col_names:\n",
    "                for t in range(1,self.t+1):\n",
    "                    new_df[col_name + '_lag' + str(t)] = self.df[col_name].shift(t)\n",
    "        else:\n",
    "            raise ValueError('Error')\n",
    "\n",
    "        # Drop the first t rows, which now contain NaN\n",
    "        return new_df.iloc[self.t:]\n",
    "\n",
    "    def __apply_windows__(self, window_size, window_stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            window_size      -   Dict passed from self.__init__\n",
    "            window_stride    -   Dict passed from self.__init__\n",
    "        Returns:    \n",
    "            n/a              -   Sets the daterange for the self.windows property to iterate along\n",
    "        \"\"\"\n",
    "        self.window_size =  {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "        self.window_stride =  {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "\n",
    "        self.window_stride.update(window_stride)\n",
    "        self.window_size.update(window_size)\n",
    "        freq = ''\n",
    "        daterangefreq = freq.join([str(v)+str(k) for (k,v) in self.window_stride.items() if v != 0])\n",
    "        self.daterange = pd.date_range(self.df.index.min(),self.df.index.max() , freq=daterangefreq)\n",
    "\n",
    "    def date_diff(self,window_size):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            window_size     -    Dict passed from self.windows function\n",
    "        Returns:\n",
    "            start_date      -    The start date of the proposed window\n",
    "            end_date        -    The end date of the proposed window    \n",
    "        \n",
    "        This function is TBC - proposed due to possible duplication of the relativedelta usage in self.windows and self.headstart\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def windows(self):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            n/a\n",
    "        Returns:\n",
    "            windows         -   Generator defining a pandas DataFrame for each window of the data. \n",
    "                                Usage like:   [window for window in LaggedTimeSeries.windows]\n",
    "        \"\"\"\n",
    "        if self.has_windows == False:\n",
    "            return self.df\n",
    "        ## Loop Over TimeSeries Range\n",
    "        for i,dt in enumerate(self.daterange):\n",
    "            \n",
    "            ## Ensure Each Division Contains Required Number of Months\n",
    "            if dt-relativedelta(years   =  self.window_size['YS'],\n",
    "                                months  =  self.window_size['MS'],\n",
    "                                days    =  self.window_size['D'],\n",
    "                                hours   =  self.window_size['H'],\n",
    "                                minutes =  self.window_size['min'],\n",
    "                                seconds =  self.window_size['S'],\n",
    "                                microseconds = self.window_size['ms']\n",
    "                                ) >= self.df.index.min():\n",
    "                \n",
    "                ## Create Window \n",
    "                yield self.df.loc[(dt-relativedelta(years   =  self.window_size['YS'],\n",
    "                                                    months  =  self.window_size['MS'],\n",
    "                                                    days    =  self.window_size['D'],\n",
    "                                                    hours   =  self.window_size['H'],\n",
    "                                                    minutes =  self.window_size['min'],\n",
    "                                                    seconds =  self.window_size['S'],\n",
    "                                                    microseconds = self.window_size['ms']\n",
    "                                                    )) : dt]\n",
    "\n",
    "    @property\n",
    "    def headstart(self):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            n/a\n",
    "        Returns:\n",
    "            len(windows)    -   The number of windows which would have start dates before the desired date range. \n",
    "                                Used in TransferEntropy class to slice off incomplete windows.\n",
    "            \n",
    "        \"\"\"\n",
    "        windows =   [i for i,dt in enumerate(self.daterange) \n",
    "                            if dt-relativedelta(    years   =  self.window_size['YS'],\n",
    "                                                    months  =  self.window_size['MS'],\n",
    "                                                    days    =  self.window_size['D'],\n",
    "                                                    hours   =  self.window_size['H'],\n",
    "                                                    minutes =  self.window_size['min'],\n",
    "                                                    seconds =  self.window_size['S'],\n",
    "                                                    microseconds = self.window_size['ms']\n",
    "                                        ) < self.df.index.min() ]\n",
    "        ## i.e. count from the first window which falls entirely after the earliest date\n",
    "        return len(windows)\n",
    "\n",
    "\n",
    "class TransferEntropy():\n",
    "    \"\"\"\n",
    "        Functional class to calculate Transfer Entropy between time series, to detect causal signals.\n",
    "        Currently accepts two series: X(t) and Y(t). Future extensions planned to accept additional endogenous \n",
    "        series: X1(t), X2(t), X3(t) etc. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, DF, endog, exog, lag = None, window_size=None, window_stride=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            DF            -   (DataFrame) Time series data for X and Y (NOT including lagged variables)\n",
    "            endog         -   (string)    Fieldname for endogenous (dependent) variable Y\n",
    "            exog          -   (string)    Fieldname for exogenous (independent) variable X\n",
    "            lag           -   (integer)   Number of periods (rows) by which to lag timeseries data\n",
    "            window_size   -   (Dict)      Must contain key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                          Describes the desired size of each window, provided the data is indexed with datetime type. Leave as\n",
    "                                          None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "            window_stride -   (Dict)      Must contain key-value pairs only from within: {'YS':0,'MS':0,'D':0,'H':0,'min':0,'S':0,'ms':0}\n",
    "                                          Describes the size of the step between consecutive windows, provided the data is indexed with datetime type. Leave as\n",
    "                                          None for no windowing. Units follow http://pandas.pydata.org/pandas-docs/stable/timeseries.html#timeseries-offset-aliases\n",
    "        Returns:\n",
    "            n/a\n",
    "        \"\"\"\n",
    "        self.lts = LaggedTimeSeries(df=sanitise(DF), \n",
    "                                    lag=lag, \n",
    "                                    window_size=window_size,\n",
    "                                    window_stride=window_stride)\n",
    "\n",
    "        if self.lts.has_windows is True:\n",
    "            self.df = self.lts.windows\n",
    "            self.date_index = self.lts.daterange[self.lts.headstart:]\n",
    "            self.results = pd.DataFrame(index=self.date_index)\n",
    "            self.results.index.name = \"windows_ending_on\"\n",
    "        else:\n",
    "            self.df = [self.lts.df]\n",
    "            self.results = pd.DataFrame(index=[0])\n",
    "        self.max_lag_only = True\n",
    "        self.endog = endog                             # Dependent Variable Y\n",
    "        self.exog = exog                               # Independent Variable X\n",
    "        self.lag = lag\n",
    "\n",
    "        \n",
    "        \"\"\" If using KDE, this ensures the covariance matrices are calculated once over all data, rather\n",
    "            than for each window. This saves computational time and provides a fair point for comparison.\"\"\"\n",
    "        self.covars = [[],[]]\n",
    "        for i,(X,Y) in enumerate({self.exog:self.endog, self.endog:self.exog}.items()):\n",
    "            X_lagged = X+'_lag'+str(self.lag)\n",
    "            Y_lagged = Y+'_lag'+str(self.lag)\n",
    "\n",
    "            self.covars[i] = [  np.cov(self.lts.df[[Y,Y_lagged,X_lagged]].values.T),\n",
    "                                np.cov(self.lts.df[[X_lagged,Y_lagged]].values.T),\n",
    "                                np.cov(self.lts.df[[Y,Y_lagged]].values.T),\n",
    "                                np.ones(shape=(1,1)) * self.lts.df[Y_lagged].std()**2 ]\n",
    "\n",
    "   \n",
    "    def nonlinear_TE(self, df=None, pdf_estimator='histogram', bins=None, bandwidth=None, gridpoints=20, n_shuffles=0):\n",
    "        \"\"\"\n",
    "        NonLinear Transfer Entropy for directional causal inference\n",
    "        Defined:            TE = TE_XY - TE_YX      where TE_XY = H(Y|Y-t) - H(Y|Y-t,X-t)\n",
    "        Calculated using:   H(Y|Y-t,X-t) = H(Y,Y-t,X-t) - H(Y,Y-t)  and finding joint entropy through density estimation\n",
    "        Arguments:\n",
    "            pdf_estimator   -   (string)    'Histogram' or 'kernel' Used to define which method is preferred for density estimation\n",
    "                                            of the distribution - either histogram or KDE\n",
    "            bins            -   (dict of lists) Optional parameter to provide hard-coded bin-edges. Dict keys \n",
    "                                            must contain names of variables - including lagged columns! Dict values must be lists\n",
    "                                            containing bin-edge numerical values. \n",
    "            bandwidth       -   (float)     Optional parameter for custom bandwidth in KDE. This is a scalar multiplier to the covariance\n",
    "                                            matrix used (see: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.covariance_factor.html)\n",
    "            gridpoints      -   (integer)   Number of gridpoints (in each dimension) to discretise the probablity space when performing\n",
    "                                            integration of the kernel density estimate. Increasing this gives more precision, but significantly\n",
    "                                            increases execution time\n",
    "            n_shuffles      -   (integer)   Number of times to shuffle the dataframe, destroying the time series temporality, in order to \n",
    "                                            perform significance testing.\n",
    "        Returns:\n",
    "            transfer_entropies  -  (list) Directional Transfer Entropies from X(t)->Y(t) and Y(t)->X(t) respectively\n",
    "        \n",
    "        (Also stores TE, Z-score and p-values in self.results - for each window if windows defined.)\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        ## Retrieve user-defined bins\n",
    "        self.bins = bins\n",
    "        if self.bins is None:\n",
    "            self.bins = {self.endog: None}\n",
    "\n",
    "        ## Prepare lists for storing results\n",
    "        TEs = []\n",
    "        shuffled_TEs = []\n",
    "        p_values = []\n",
    "        z_scores = []\n",
    "\n",
    "        ## Loop over all windows\n",
    "        for i,df in enumerate(self.df):\n",
    "            df = deepcopy(df)\n",
    "\n",
    "            ## Shows user that something is happening\n",
    "            if self.lts.has_windows is True:\n",
    "                print(\"Window ending: \", self.date_index[i])\n",
    "\n",
    "            ## Initialise list to return TEs\n",
    "            transfer_entropies = [0,0]\n",
    "\n",
    "            ## Require us to compare information transfer bidirectionally\n",
    "            for i,(X,Y) in enumerate({self.exog:self.endog, self.endog:self.exog}.items()):\n",
    "                \n",
    "                ### Entropy calculated using Probability Density Estimation:\n",
    "                    # Following: https://stat.ethz.ch/education/semesters/SS_2006/CompStat/sk-ch2.pdf\n",
    "                    # Also: https://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs/lec5.pdf\n",
    "                \n",
    "                ## Note Lagged Terms\n",
    "                X_lagged = X+'_lag'+str(self.lag)\n",
    "                Y_lagged = Y+'_lag'+str(self.lag)\n",
    "\n",
    "                ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "\n",
    "                ## 1. H(Y,Y-t,X-t)  \n",
    "                H1 = get_entropy(df = df[[Y,Y_lagged,X_lagged]], \n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth = bandwidth, \n",
    "                                estimator = pdf_estimator,\n",
    "                                bins = {k:v for (k,v) in self.bins.items()\n",
    "                                        if k in[Y,Y_lagged,X_lagged]},\n",
    "                                covar = self.covars[i][0])\n",
    "                ## 2. H(Y-t,X-t)\n",
    "                H2 = get_entropy(df = df[[X_lagged,Y_lagged]],\n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth = bandwidth,\n",
    "                                estimator = pdf_estimator,\n",
    "                                bins = {k:v for (k,v) in self.bins.items() \n",
    "                                        if k in [X_lagged,Y_lagged]},\n",
    "                                covar = self.covars[i][1]) \n",
    "                ## 3. H(Y,Y-t)  \n",
    "                H3 = get_entropy(df = df[[Y,Y_lagged]],\n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth  = bandwidth,\n",
    "                                estimator = pdf_estimator,\n",
    "                                bins =  {k:v for (k,v) in self.bins.items() \n",
    "                                        if k in [Y,Y_lagged]},\n",
    "                                covar = self.covars[i][2])\n",
    "                ## 4. H(Y-t)  \n",
    "                H4 = get_entropy(df = df[[Y_lagged]],\n",
    "                                gridpoints = gridpoints,\n",
    "                                bandwidth  = bandwidth,\n",
    "                                estimator = pdf_estimator,\n",
    "                                bins =  {k:v for (k,v) in self.bins.items() \n",
    "                                        if k in [Y_lagged]},\n",
    "                                covar = self.covars[i][3])                \n",
    "\n",
    "\n",
    "                ### Calculate Conditonal Entropy using: H(Y|X-t,Y-t) = H(Y,X-t,Y-t) - H(X-t,Y-t)\n",
    "                conditional_entropy_joint =  H1 - H2\n",
    "            \n",
    "                ### And Conditional Entropy independent of X(t) H(Y|Y-t) = H(Y,Y-t) - H(Y-t)            \n",
    "                conditional_entropy_independent = H3 - H4\n",
    "\n",
    "                ### Directional Transfer Entropy is the difference between the conditional entropies\n",
    "                transfer_entropies[i] =  conditional_entropy_independent - conditional_entropy_joint\n",
    "            \n",
    "            TEs.append(transfer_entropies)\n",
    "\n",
    "        ## Store Transfer Entropy from X(t)->Y(t) and from Y(t)->X(t)\n",
    "        self.add_results({'TE_XY' : np.array(TEs)[:,0],\n",
    "                          'TE_YX' : np.array(TEs)[:,1],\n",
    "                          'p_value_XY' : None,\n",
    "                          'p_value_YX' : None,\n",
    "                          'z_score_XY' : 0,\n",
    "                          'z_score_YX' : 0\n",
    "                          })\n",
    "        \n",
    "        return transfer_entropies\n",
    "\n",
    "    def add_results(self,dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dict    -   JSON-style data to store in existing self.results DataFrame\n",
    "        Returns:\n",
    "            n/a\n",
    "        \"\"\"\n",
    "        for (k,v) in dict.items():\n",
    "            self.results[str(k)] = v\n",
    "\n",
    "\n",
    "def get_entropy(df, gridpoints=15, bandwidth=None, estimator='kernel', bins=None, covar=None):\n",
    "    \"\"\"\n",
    "        Function for calculating entropy from a probability mass \n",
    "        \n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                        the domain. Used if estimator='kernel'\n",
    "        bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                        matrix). Used if estimator='kernel'\n",
    "        estimator   -       (string)    'histogram' or 'kernel'\n",
    "        bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator\n",
    "                                        = 'histogram'\n",
    "        covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                        Used if estimator = 'kernel'\n",
    "    Returns:\n",
    "        entropy     -       (float)     Shannon entropy in bits\n",
    "    \"\"\"\n",
    "    pdf = get_pdf(df, gridpoints, bandwidth, estimator, bins, covar)\n",
    "    ## log base 2 returns H(X) in bits\n",
    "    return -np.sum( pdf * ma.log2(pdf).filled(0)) \n",
    "\n",
    "def shuffle_series(DF, only=None):\n",
    "    \"\"\"\n",
    "    Function to return time series shuffled rowwise along each desired column. \n",
    "    Each column is shuffled independently, removing the temporal relationship.\n",
    "    This is to calculate Z-score and Z*-score. See P. Boba et al (2015)\n",
    "    Calculated using:       df.apply(np.random.permutation)\n",
    "    Arguments:\n",
    "        df              -   (DataFrame) Time series data \n",
    "        only            -   (list)      Fieldnames to shuffle. If none, all columns shuffled \n",
    "    Returns:\n",
    "        df_shuffled     -   (DataFrame) Time series shuffled along desired columns    \n",
    "    \"\"\"\n",
    "    if not only == None:\n",
    "        shuffled_DF = DF.copy()\n",
    "        for col in only:\n",
    "            series = DF.loc[:, col].to_frame()\n",
    "            shuffled_DF[col] = series.apply(np.random.permutation)\n",
    "    else:\n",
    "        shuffled_DF = DF.apply(np.random.permutation)\n",
    "    \n",
    "    return shuffled_DF\n",
    "\n",
    "def get_pdf(df, gridpoints=None, bandwidth=None, estimator=None, bins=None, covar=None):\n",
    "    \"\"\"\n",
    "        Function for non-parametric density estimation\n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        gridpoints  -       (int)       Number of gridpoints when integrating KDE over \n",
    "                                        the domain. Used if estimator='kernel'\n",
    "        bandwidth   -       (float)     Bandwidth for KDE (scalar multiple to covariance\n",
    "                                        matrix). Used if estimator='kernel'\n",
    "        estimator   -       (string)    'histogram' or 'kernel'\n",
    "        bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator = 'histogram'\n",
    "        covar       -       (Numpy ndarray) Covariance matrix between dimensions of df. \n",
    "                                        Used if estimator = 'kernel'\n",
    "    Returns:\n",
    "        pdf         -       (Numpy ndarray) Probability of a sample being in a specific \n",
    "                                        bin (technically a probability mass)\n",
    "    \"\"\"\n",
    "\n",
    "    pdf = pdf_histogram(df, bins)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "\n",
    "def pdf_histogram(df,bins):\n",
    "    \"\"\"\n",
    "        Function for non-parametric density estimation using N-Dimensional Histograms\n",
    "    Args:\n",
    "        df            -       (DataFrame) Samples over which to estimate density\n",
    "        bins          -       (Dict of lists) Bin edges for NDHistogram. \n",
    "    Returns:\n",
    "        histogram.pdf -       (Numpy ndarray) Probability of a sample being in a specific \n",
    "                                    bin (technically a probability mass)\n",
    "    \"\"\"\n",
    "    histogram = NDHistogram(df=df, bins=bins)        \n",
    "    return histogram.pdf\n",
    "\n",
    "def plot_pdf_histogram(df, bins, cmap='inferno'):\n",
    "    \"\"\"\n",
    "    Function to plot the pdf of a dataset, estimated via histogram.\n",
    "        \n",
    "    Args:\n",
    "        df          -       (DataFrame) Samples over which to estimate density\n",
    "        bins        -       (Dict of lists) Bin edges for NDHistogram. Used if estimator = 'histogram'\n",
    "    Returns:\n",
    "        ax          -       AxesSubplot object, passed back via to plot_pdf() function\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ## Calculate PDF\n",
    "    PDF = get_pdf(df=df, bins=bins)\n",
    "\n",
    "    ## Get x-coords, y-coords for each bar\n",
    "    (x_edges,y_edges) = bins.values()\n",
    "    X, Y = np.meshgrid(x_edges[:-1], y_edges[:-1])\n",
    "    ## Get dx, dy for each bar\n",
    "    dxs, dys = np.meshgrid(np.diff(x_edges),np.diff(y_edges))\n",
    "\n",
    "    ## Colourmap\n",
    "    cmap = cm.get_cmap(cmap) \n",
    "    rgba = [cmap((p-PDF.flatten().min())/PDF.flatten().max()) for p in PDF.flatten()] \n",
    "\n",
    "    ## Create subplots\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.bar3d(   x = X.flatten(),            #x coordinates of each bar\n",
    "                y = Y.flatten(),            #y coordinates of each bar\n",
    "                z = 0,                      #z coordinates of each bar\n",
    "                dx = dxs.flatten(),         #width of each bar\n",
    "                dy = dys.flatten(),         #depth of each bar\n",
    "                dz = PDF.flatten() ,        #height of each bar\n",
    "                alpha = 1,                  #transparency\n",
    "                color = rgba\n",
    "    )\n",
    "    ax.set_title(\"Histogram Probability Distribution\",fontsize=10)\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1221fa08-25e5-4af9-a288-4c257ecc73fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0, -0.08404122626173804]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-c1876acb3630>:13: DeprecationWarning: Passing normed=False is deprecated, and has no effect. Consider passing the density argument instead.\n",
      "  hist, dedges = np.histogram(df.values, bins=ordered_bins[0], normed=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_entropy(df, bins=None):\n",
    "    pdf = pdf_histogram(df, bins)\n",
    "    ## log base 2 returns H(X) in bits\n",
    "    return -np.sum( pdf * ma.log2(pdf).filled(0)) \n",
    "\n",
    "\n",
    "def pdf_histogram(df, bins):\n",
    "    ordered_bins = [sorted(bins[key]) for key in sorted(bins.keys())]\n",
    "    hist, dedges = np.histogram(df.values, bins=ordered_bins[0], normed=False)\n",
    "    return hist/hist.sum()\n",
    "\n",
    "\n",
    "def sigma_bins(df, lags,  max_bins=15):\n",
    "    \"\"\" \n",
    "    Returns bins for N-dimensional data, using standard deviation binning: each \n",
    "    bin is one S.D in width, with bins centered on the mean. Where outliers exist \n",
    "    beyond the maximum number of SDs dictated by the max_bins parameter, the\n",
    "    bins are extended to minimum/maximum values to ensure all data points are\n",
    "    captured. This may mean larger bins in the tails, and up to two bins \n",
    "    greater than the max_bins parameter suggests in total (in the unlikely case of huge\n",
    "    outliers on both sides). \n",
    "    Args:\n",
    "        max_bins        -   (int)       The maximum allowed bins in each dimension\n",
    "    Returns:\n",
    "        bins            -   (dict)      The optimal bin-edges for pdf estimation\n",
    "                                        using the histogram method, keyed by df column names\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    bins = {k:[np.mean(v)-int(max_bins/2)*np.std(v) + i * np.std(v) for i in range(max_bins+1)] \n",
    "            for (k,v) in df.iteritems()}   # Note: same as:  self.df.to_dict('list').items()}\n",
    "\n",
    "    # Since some outliers can be missed, extend bins if any points are not yet captured\n",
    "    [bins[k].append(df[k].min()) for k in df.keys() if df[k].min() < min(bins[k])]\n",
    "    [bins[k].append(df[k].max()) for k in df.keys() if df[k].max() > max(bins[k])]\n",
    "\n",
    "    bins.update({fieldname + '_lag' + str(t): edges          \n",
    "                        for (fieldname, edges) in bins.items() for t in range(lags)})\n",
    "    return bins\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/results_day_binned_with_states.csv\")\n",
    "endog = 'Anti-Regulation Fear-of-Regulation'\n",
    "exog = 'Daily Background Checks'\n",
    "df = df[[exog, endog]]\n",
    "for col_name in list(df.columns):\n",
    "    for t in range(1, lag + 1):\n",
    "        df[col_name + '_lag' + str(t)] = df[col_name].shift(t)\n",
    "\n",
    "lag = 5\n",
    "bins = sigma_bins(df, lag)\n",
    "TEs = []\n",
    "shuffled_TEs = []\n",
    "p_values = []\n",
    "z_scores = []\n",
    "\n",
    "## Initialise list to return TEs\n",
    "transfer_entropies = [0,0]\n",
    "\n",
    "## Require us to compare information transfer bidirectionally\n",
    "for i, (X, Y) in enumerate({exog:endog, endog:exog}.items()):\n",
    "\n",
    "    ### Entropy calculated using Probability Density Estimation:\n",
    "        # Following: https://stat.ethz.ch/education/semesters/SS_2006/CompStat/sk-ch2.pdf\n",
    "        # Also: https://www.cs.cmu.edu/~aarti/Class/10704_Spring15/lecs/lec5.pdf\n",
    "\n",
    "    ## Note Lagged Terms\n",
    "    X_lagged = X + '_lag' + str(lag)\n",
    "    Y_lagged = Y + '_lag' + str(lag)\n",
    "\n",
    "    ### Estimate PDF using Gaussian Kernels and use H(x) = p(x) log p(x)\n",
    "\n",
    "    ## 1. H(Y,Y-t,X-t)  \n",
    "    H1 = get_entropy(df = df[[Y, Y_lagged, X_lagged]], \n",
    "                    bins = {k:v for (k,v) in bins.items()\n",
    "                            if k in[Y,Y_lagged,X_lagged]})\n",
    "    ## 2. H(Y-t,X-t)\n",
    "    H2 = get_entropy(df = df[[X_lagged,Y_lagged]],\n",
    "                    bins = {k:v for (k,v) in bins.items() \n",
    "                            if k in [X_lagged,Y_lagged]}) \n",
    "    ## 3. H(Y,Y-t)  \n",
    "    H3 = get_entropy(df = df[[Y,Y_lagged]],\n",
    "                    bins =  {k:v for (k,v) in bins.items() \n",
    "                            if k in [Y,Y_lagged]})\n",
    "    ## 4. H(Y-t)  \n",
    "    H4 = get_entropy(df = df[[Y_lagged]],\n",
    "                    bins =  {k:v for (k,v) in bins.items() \n",
    "                            if k in [Y_lagged]})                \n",
    "\n",
    "\n",
    "    ### Calculate Conditonal Entropy using: H(Y|X-t,Y-t) = H(Y,X-t,Y-t) - H(X-t,Y-t)\n",
    "    conditional_entropy_joint =  H1 - H2\n",
    "\n",
    "    ### And Conditional Entropy independent of X(t) H(Y|Y-t) = H(Y,Y-t) - H(Y-t)            \n",
    "    conditional_entropy_independent = H3 - H4\n",
    "\n",
    "    ### Directional Transfer Entropy is the difference between the conditional entropies\n",
    "    transfer_entropies[i] =  conditional_entropy_independent - conditional_entropy_joint\n",
    "\n",
    "TEs.append(transfer_entropies)\n",
    "\n",
    "print(TEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fab470a1-2c18-4942-b49f-4859e6c9987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-110b6f7736a9>:52: UserWarning: Incompatible bins provided - defaulting to sigma bins\n",
      "  warnings.warn('Incompatible bins provided - defaulting to sigma bins')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoBins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-497dbf0788e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_pdf_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-110b6f7736a9>\u001b[0m in \u001b[0;36mplot_pdf_histogram\u001b[0;34m(df, bins, cmap)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \u001b[0;31m## Calculate PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0mPDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;31m## Get x-coords, y-coords for each bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-110b6f7736a9>\u001b[0m in \u001b[0;36mget_pdf\u001b[0;34m(df, gridpoints, bandwidth, estimator, bins, covar)\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m     \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdf_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-110b6f7736a9>\u001b[0m in \u001b[0;36mpdf_histogram\u001b[0;34m(df, bins)\u001b[0m\n\u001b[1;32m    515\u001b[0m                                     \u001b[0mbin\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtechnically\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mmass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \"\"\"\n\u001b[0;32m--> 517\u001b[0;31m     \u001b[0mhistogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNDHistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistogram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-110b6f7736a9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, df, bins, max_bins)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Incompatible bins provided - defaulting to sigma bins'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mAB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoBins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_bins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoBins' is not defined"
     ]
    }
   ],
   "source": [
    "plot_pdf_histogram(df, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fa1d0a3-f7e5-4297-ae30-a127d2c4a57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scipy\n",
      "  Downloading scipy-1.7.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 28.4 MB 24.1 MB/s eta 0:00:01    |██████████████████████████▊     | 23.7 MB 2.0 MB/s eta 0:00:03\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /home/kslote/anaconda3/lib/python3.8/site-packages (from scipy) (1.20.1)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.7.0\n",
      "\u001b[33mWARNING: You are using pip version 21.1.1; however, version 21.1.3 is available.\n",
      "You should consider upgrading via the '/home/kslote/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
